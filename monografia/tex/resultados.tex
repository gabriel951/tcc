Nesta capítulo, mostram-se as melhores configurações para os modelos de aprendizagem
de máquina estudados. Em seguida, apresentam-se os resultados obtidos para as
diversas técnicas de aprendizagem de máquina consideradas. Por fim, analisam-se os
resultados obtidos. 

\section{Configurações Obtidas Para Modelos de Aprendizagem de Máquina}
\label{conf_ml_models}
Nesta seção, analisa-se quais foram as melhores configurações obtidas para os
seguintes métodos de mineração de dados: ANN, SVR e \textit{Naive Bayes}. 

\subsection{Configurações da ANN} \label{ann_best_conf}
Inicialmente, estudou-se qual seria a quantidade ótima de neurônios da camada
escondida da rede neural.
Depois, estudou-se qual seria o melhor valor para o parâmetro taxa de aprendizagem. 
Mostrou-se também o desempenho do melhor modelo no conjunto de validação. 
Os resultados estão disponíveis na Tabela \ref{conf_ann}. 

\begin{table}
\caption{Melhor Escolha de Parâmetros Para ANN}
\begin{center}
\begin{tabular}[c]{| c | c | c | c |}
    \hline
    \textbf{Dados} & \textbf{Neurônios} & \textbf{Aprendizagem} 
    & \textbf{Desempenho}\\
    \hline
    Alunos Jovens da FT & 100 & 0.001 & 0.81 \\
    \hline
    Alunos Jovens da Licenciatura & 36 & 1.0 & 0.79 \\
    \hline
    Alunos Jovens da Computação & 36 & 0.001 & 0.77 \\
    \hline
    Alunos Seniores & 24  & 0.7 & 0.77 \\
    \hline
\end{tabular}
\end{center}
\label{conf_ann}
\end{table}

Os resultados da Tabela \ref{conf_ann} evidenciam que a melhor configuração, tanto
para o número de neurônios quanto para a taxa de aprendizagem, varia conforme a base
de dados em questão. 

\subsection{Configurações da SVR} \label{svr_best_conf}
Estudou-se como o tipo de \textit{kernel} e o parâmetro de penalização
poderiam ser escolhidos de modo a obter uma boa configuração para a SVR. Mostrou-se
também o desempenho do melhor modelo no conjunto de validação. Os
resultados são apresentados na Tabela \ref{svr_conf}. 

\begin{table}
\caption{Melhor Escolha de Parâmetros Para SVR}
\begin{center}
\begin{tabular}[c]{| c | c | c | c |}
    \hline
    \textbf{Dados} & \textbf{Kernel} & \textbf{Penalização} & \textbf{Desempenho}\\
    \hline
    Alunos Jovens da FT & linear & 1.0 & 0.79 \\
    \hline
    Alunos Jovens da Licenciatura & linear & 1.0 & 0.81 \\
    \hline
    Alunos Jovens da Computação & rbf & 1.0 & 0.83 \\
    \hline
    Alunos Seniores & linear & 1.0 & 0.67 \\
    \hline
\end{tabular}
\end{center}
\label{svr_conf}
\end{table}

Os resultados da Tabela \ref{svr_conf} mostram que as melhores configurações não
dependem muito da base de dados com a qual estamos trabalhando. Em três das quatro
bases de dados, o tipo de \textit{kernel} escolhido foi o linear. Em todas as bases
de dados o valor para o parâmetro de penalização foi $C = 1.0$.   

\subsection{Configuração do Naive Bayes} \label{nb_best_conf}
Estudou-se como o tipo de distribuição assumida por cada atributo influenciou o
desempenho no método \textit{Naive Bayes}. Os resultados são mostrados na Tabela
\ref{conf_nb} e evidenciam que a melhor escolha para o tipo de distribuição é
bastante dependente da base de dados com a qual estamos trabalhando. Mostra-se também
o desempenho do modelo induzido no conjunto de validação. 

\begin{table}
\caption{Melhor Escolha de Parâmetros Para Naive Bayes}
\begin{center}
\begin{tabular}[c]{| c | c | c | c |}
    \hline
    \textbf{Dados} & \textbf{Atributos} & \textbf{Desempenho} \\
    \hline
    Alunos Jovens da FT & Gaussiana & 0.63 \\
    \hline
    Alunos Jovens da Licenciatura & Bernoulli & 0.78 \\
    \hline
    Alunos Jovens da Computação & Multinomial & 0.72 \\
    \hline
    Alunos Seniores & Gaussiana & 0.64 \\
    \hline
\end{tabular}
\end{center}
\label{conf_nb}
\end{table}

\section{Desempenho dos Modelos de Aprendizagem de Máquina} \label{results_ml_models}
Apresenta-se nesta seção os desempenhos dos vários modelos induzidos 
para, em seguida, discutir os resultados obtidos. Nas Tabelas
\ref{fmeasure_ft}, \ref{fmeasure_lic}, \ref{fmeasure_comp} e \ref{fmeasure_old} são 
mostrados os desempenhos para as bases de dados dos alunos jovens da FT, alunos
jovens da licenciatura, alunos jovens da computação e alunos seniores
respectivamente. O valor de \textit{F-measure} mostrado corresponde a média dos valores de
\textit{F-measure} obtidos semestre a semestre.  

\begin{table}
\caption{F-measure Média por Modelo, para Alunos Jovens da FT}
\begin{center}
\begin{tabular}[c]{| c | c |}
    \hline
    \textbf{Modelo} & \textbf{F-measure} \\
    \hline
    ANN              & 0.76 \\
    \hline
    Naive Bayes      & 0.56 \\
    \hline
    Random Forest    & 0.73 \\
    \hline
    Regressor Linear & 0.80 \\
    \hline
    SVR              & 0.76 \\
    \hline
    ZeroR            & 0.64 \\
    \hline
\end{tabular}
\end{center}
\label{fmeasure_ft}
\end{table}

\begin{table}
\caption{F-measure Média por Modelo, para Alunos Jovens de Licenciatura}
\begin{center}
\begin{tabular}[c]{| c | c |}
    \hline
    \textbf{Modelo} & \textbf{F-measure} \\
    \hline
    ANN              & 0.85 \\
    \hline
    Naive Bayes      & 0.76 \\
    \hline
    Random Forest    & 0.85 \\
    \hline
    Regressor Linear & 0.86 \\
    \hline
    SVR              & 0.82 \\
    \hline
    ZeroR            & 0.70 \\
    \hline
\end{tabular}
\end{center}
\label{fmeasure_lic}
\end{table}

\begin{table}
\caption{F-measure Média por Modelo, para Alunos Jovens da Computação}
\begin{center}
\begin{tabular}[c]{| c | c |}
    \hline
    \textbf{Modelo} & \textbf{F-measure} \\
    \hline
    ANN              & 0.74 \\
    \hline
    Naive Bayes      & 0.65 \\
    \hline
    Random Forest    & 0.76 \\
    \hline
    Regressor Linear & 0.77 \\
    \hline
    SVR              & 0.70 \\
    \hline
    ZeroR            & 0.60 \\
    \hline
\end{tabular}
\end{center}
\label{fmeasure_comp}
\end{table}

\begin{table}
\caption{F-measure Média por Modelo, para Alunos Seniores}
\begin{center}
\begin{tabular}[c]{| c | c |}
    \hline
    \textbf{Modelo} & \textbf{F-measure} \\
    \hline
    ANN              & 0.62 \\
    \hline
    Naive Bayes      & 0.28 \\
    \hline
    Random Forest    & 0.70 \\
    \hline
    Regressor Linear & 0.75 \\
    \hline
    SVR              & 0.79 \\
    \hline
    ZeroR            & 0.61 \\
    \hline
\end{tabular}
\end{center}
\label{fmeasure_old}
\end{table}

\par Os resultados mostram que, de forma geral, os algoritmos de aprendizagem de máquina
conseguem ter um resultado melhor que o ZeroR (à exceção do \textit{Naive Bayes}). O melhor
desempenho dos modelos em relação ao ZeroR era esperado, já que o ZeroR é bastante
simples. Atribui-se o mau desempenho do \textit{Naive Bayes} ao
fato de que os atributos passados não poderem ser considerados condicionalmente
independentes, premissa admitida para o uso do algoritmo. 
\par Por fim, deve-se ressaltar o bom desempenho obtido pelo regressor linear. Tal
técnica obteve o melhor desempenho em três das quatro bases de dados (perdendo apenas
para a SVR na base de dados dos alunos seniores) e obteve um valor de
\textit{F-measure} médio em torno de 0.795. Isso está de acordo com a teoria de
aprendizagem de máquina, que afirma que modelos lineares não são propensos à
\textit{overfitting} e que são boas alternativas iniciais em geral
\cite{ml_second_book}. 
